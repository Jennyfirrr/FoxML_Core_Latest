# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial
# Copyright (c) 2025-2026 Fox ML Infrastructure LLC

"""
Shared Ranking Harness

Unified harness for both target ranking and feature selection that ensures:
- Same split policy (PurgedTimeSeriesSplit with time-based purging)
- Same model evaluation (train_and_evaluate_models)
- Same telemetry (RunContext, reproducibility tracking)
- Same data sanitization and dtype canonicalization

This prevents "in-sample-ish" mistakes from scaling and ensures both paths
use identical evaluation contracts.
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import pandas as pd

# SST: Import View and Stage enums for consistent view/stage handling
from TRAINING.orchestration.utils.scope_resolution import View, Stage
# DETERMINISM: Import deterministic iteration helpers
from TRAINING.common.utils.determinism_ordering import sorted_unique

logger = logging.getLogger(__name__)


class RankingHarness:
    """
    Shared harness for ranking operations (target ranking and feature selection).
    
    Ensures both use:
    - Same split generator (walk-forward / purged, same embargo, same grouping)
    - Same scoring function + metric normalization
    - Same leakage-safe imputation policy
    - Same RunContext + reproducibility tracker payload
    - Same logging/artifact writer
    """
    
    def __init__(
        self,
        job_type: str,  # "rank_targets" or "rank_features"
        target_column: str,
        symbols: List[str],
        data_dir: Path,
        model_families: List[str],
        multi_model_config: Dict[str, Any] = None,
        output_dir: Optional[Path] = None,
        view: Union[str, View] = View.CROSS_SECTIONAL,  # View enum or "CROSS_SECTIONAL", "SYMBOL_SPECIFIC", "LOSO"
        symbol: Optional[str] = None,  # Required for SYMBOL_SPECIFIC and LOSO views
        explicit_interval: Optional[Union[int, str]] = None,
        experiment_config: Optional[Any] = None,
        min_cs: Optional[int] = None,
        max_cs_samples: Optional[int] = None,
        max_rows_per_symbol: Optional[int] = None
    ):
        """
        Initialize the ranking harness.
        
        Args:
            job_type: "rank_targets" or "rank_features"
            target_column: Target column name
            symbols: List of symbols to process
            data_dir: Directory containing symbol data
            model_families: List of model family names
            multi_model_config: Multi-model configuration dict
            output_dir: Optional output directory for results
            view: Evaluation view ("CROSS_SECTIONAL", "SYMBOL_SPECIFIC", "LOSO")
            symbol: Symbol name (required for SYMBOL_SPECIFIC and LOSO views)
            explicit_interval: Explicit data interval (e.g., "5m" or 5)
            experiment_config: Optional ExperimentConfig
            min_cs: Minimum cross-sectional samples
            max_cs_samples: Maximum cross-sectional samples
            max_rows_per_symbol: Maximum rows per symbol
        """
        self.job_type = job_type
        self.target_column = target_column
        self.symbols = symbols
        self.data_dir = data_dir
        self.model_families = model_families
        self.multi_model_config = multi_model_config or {}
        self.output_dir = output_dir
        self.view = view
        self.symbol = symbol
        self.explicit_interval = explicit_interval
        self.experiment_config = experiment_config
        self.min_cs = min_cs
        self.max_cs_samples = max_cs_samples
        self.max_rows_per_symbol = max_rows_per_symbol
        
        # SST: Compute universe_sig for consistent artifact scoping
        self.universe_sig = None
        if self.symbols:
            try:
                from TRAINING.orchestration.utils.run_context import compute_universe_signature
                self.universe_sig = compute_universe_signature(self.symbols)
            except Exception:
                pass
        
        # Normalize view to enum for validation
        view_enum = View.from_string(view) if isinstance(view, str) else view
        self.view = view_enum  # Store as enum internally
        
        # Validate view and symbol parameters
        if view_enum == View.SYMBOL_SPECIFIC and symbol is None:
            raise ValueError(f"symbol parameter required for View.SYMBOL_SPECIFIC view")
        # LOSO is not a View enum value, check as string
        if isinstance(view, str) and view == "LOSO" and symbol is None:
            raise ValueError(f"symbol parameter required for LOSO view")
        if view_enum == View.CROSS_SECTIONAL and symbol is not None:
            logger.warning(f"symbol={symbol} provided but view=View.CROSS_SECTIONAL, ignoring symbol")
            self.symbol = None
    
    def build_panel(
        self,
        target_column: str,
        target: Optional[str] = None,
        feature_names: Optional[List[str]] = None,
        use_strict_registry: bool = False  # If True, use strict registry mode (for feature selection), else permissive (for target ranking)
    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[List[str]], 
               Optional[np.ndarray], Optional[np.ndarray], Optional[Dict[str, pd.DataFrame]], 
               Optional[float], Optional[Any], Optional[Dict[str, Any]]]:
        """
        Build panel data (X, y, feature_names, symbols_array, time_vals, mtf_data, detected_interval, resolved_config, resolved_data_config).
        
        This is the same data build logic used by target ranking, including:
        - Target-conditional exclusions
        - Leakage filtering with registry validation
        - Resolved config creation
        
        Args:
            target_column: Target column name
            target: Optional target name (for exclusion list generation)
            feature_names: Optional list of feature names to use (None = all safe features)
        
        Returns:
            Tuple of 9 values: (X, y, feature_names, symbols_array, time_vals, mtf_data, detected_interval, resolved_config, resolved_data_config)
            - X: Feature matrix (np.ndarray)
            - y: Target array (np.ndarray)
            - feature_names: List of feature names
            - symbols_array: Symbol array (np.ndarray)
            - time_vals: Timestamp array (np.ndarray)
            - mtf_data: Multi-timeframe data dict (Dict[str, pd.DataFrame])
            - detected_interval: Detected data interval in minutes (float)
        
        PERFORMANCE AUDIT: This function is tracked for call counts and timing.
            - resolved_config: ResolvedConfig object with purge/embargo settings
            - resolved_data_config: Dict with resolved data mode and loader contract
            Any can be None if data preparation fails
        """
        from TRAINING.ranking.utils.cross_sectional_data import (
            load_mtf_data_for_ranking,
            prepare_cross_sectional_data_for_ranking
        )
        from TRAINING.ranking.utils.leakage_filtering import filter_features_for_target, _extract_horizon, _load_leakage_config
        from TRAINING.ranking.utils.data_interval import detect_interval_from_dataframe
        from TRAINING.ranking.utils.target_conditional_exclusions import (
            generate_target_exclusion_list,
            load_target_exclusion_list
        )
        from TRAINING.ranking.utils.resolved_config import create_resolved_config
        from TRAINING.ranking.predictability.scoring import TaskType
        
        # PERFORMANCE AUDIT: Track build_panel calls
        import time
        build_start_time = time.time()
        try:
            from TRAINING.common.utils.performance_audit import get_auditor
            auditor = get_auditor()
            if auditor.enabled:
                fingerprint_kwargs = {
                    'target': target_column,
                    'n_symbols': len(self.symbols),
                    'view': self.view,
                    'symbol': self.symbol,
                    'n_features_requested': len(feature_names) if feature_names else None
                }
                fingerprint = auditor._compute_fingerprint('RankingHarness.build_panel', **fingerprint_kwargs)
        except Exception:
            auditor = None
            fingerprint = None
        
        # Filter symbols based on view
        symbols_to_load = self.symbols
        # self.view is now a View enum after normalization in __init__
        if self.view == View.SYMBOL_SPECIFIC:
            symbols_to_load = [self.symbol]
        elif isinstance(self.view, str) and self.view == "LOSO":
            # LOSO: train on all symbols except symbol, validate on symbol
            symbols_to_load = [s for s in self.symbols if s != self.symbol]
            validation_symbol = self.symbol
        else:
            validation_symbol = None
        
        # ========================================================================
        # COLUMN PROJECTION: Skip lazy loading when caller provides feature_names
        # ========================================================================
        # If caller provided feature_names, use them directly for column projection
        # This skips preflight/probe entirely (caller already knows safe features)
        if feature_names is not None:
            columns_to_load = list(feature_names)
            # Add metadata columns
            metadata_cols = ['ts', 'symbol', 'date', 'time', target_column]
            for col in metadata_cols:
                if col not in columns_to_load:
                    columns_to_load.append(col)
            columns_to_load = sorted(set(columns_to_load))
            logger.info(f"üìä Using caller-provided features for column projection: {len(columns_to_load)} columns")
        else:
            # ========================================================================
            # LAZY LOADING: Preflight + Probe for memory optimization
            # ========================================================================
            columns_to_load = None  # None = load all (backward compatible)

            logger.info(f"üîç [LAZY_DEBUG] Checking lazy loading config for {len(symbols_to_load)} symbols...")

        # Only run lazy loading detection if feature_names was not provided
        if feature_names is None:
            try:
                from CONFIG.config_loader import get_cfg

                # First try experiment config (has precedence)
                lazy_enabled = False
                probe_enabled = True
                probe_top_n = 100
                probe_rows = 10000

                if self.experiment_config:
                    # Check experiment config directly (higher precedence)
                    exp_name = self.experiment_config.name if hasattr(self.experiment_config, 'name') else str(self.experiment_config)
                    try:
                        exp_file = Path("CONFIG/experiments") / f"{exp_name}.yaml"
                        if exp_file.exists():
                            import yaml
                            with open(exp_file, 'r') as f:
                                exp_yaml = yaml.safe_load(f) or {}
                            lazy_cfg = exp_yaml.get('intelligent_training', {}).get('lazy_loading', {})
                            if lazy_cfg:
                                lazy_enabled = lazy_cfg.get('enabled', False)
                                probe_enabled = lazy_cfg.get('probe_features', True)
                                probe_top_n = int(lazy_cfg.get('probe_top_n', 100))
                                probe_rows = int(lazy_cfg.get('probe_rows', 10000))
                                logger.info(f"üîç [LAZY_DEBUG] Loaded from experiment config: {exp_name}")
                    except Exception as e:
                        logger.debug(f"Could not load experiment config: {e}")

                # Fallback to base config if not set by experiment
                if not lazy_enabled:
                    lazy_enabled = get_cfg("intelligent_training.lazy_loading.enabled", default=False)
                    probe_enabled = get_cfg("intelligent_training.lazy_loading.probe_features", default=True)
                    probe_top_n = int(get_cfg("intelligent_training.lazy_loading.probe_top_n", default=100))
                    probe_rows = int(get_cfg("intelligent_training.lazy_loading.probe_rows", default=10000))

                logger.info(f"üîç [LAZY_DEBUG] Config loaded: lazy_enabled={lazy_enabled}, probe_enabled={probe_enabled}, "
                            f"probe_top_n={probe_top_n}, symbols={len(symbols_to_load)}")

                # OPTIMIZATION: Always use lazy loading when enabled (removed >50 symbol threshold)
                # Column projection should happen regardless of symbol count
                use_lazy = lazy_enabled

                if not use_lazy:
                    logger.info(f"üîç [LAZY_DEBUG] Lazy loading SKIPPED: lazy_enabled={lazy_enabled}")

                if use_lazy and target_column:
                    logger.info(f"üéØ Lazy loading enabled for {len(symbols_to_load)} symbols")

                    try:
                        from TRAINING.ranking.utils.preflight_leakage import preflight_filter_features
                        from TRAINING.data.loading.unified_loader import UnifiedDataLoader

                        # Get interval from data_dir path
                        interval = "5m"
                        interval_minutes = 5
                        data_dir_str = str(self.data_dir)
                        if "interval=" in data_dir_str:
                            import re
                            match = re.search(r"interval=(\d+)([mhd]?)", data_dir_str)
                            if match:
                                interval = match.group(1) + (match.group(2) or 'm')
                                interval_minutes = int(match.group(1))

                        loader = UnifiedDataLoader(data_dir=self.data_dir, interval=interval)

                        # Preflight uses data_dir and symbols to read schema internally
                        # DETERMINISM: Sort symbols before sampling to ensure consistent preflight results
                        preflight_result = preflight_filter_features(
                            data_dir=self.data_dir,
                            symbols=sorted(symbols_to_load)[:20],  # Sample for schema (sorted for determinism)
                            targets=[target_column],
                            interval_minutes=interval_minutes,
                            for_ranking=True,
                            verbose=False,
                        )
                        preflight_features = preflight_result.get(target_column, [])

                        if preflight_features:
                            logger.info(f"   üìã Preflight: schema ‚Üí {len(preflight_features)} safe columns")

                            if probe_enabled and len(preflight_features) > probe_top_n:
                                from TRAINING.ranking.utils.feature_probe import probe_features_for_target

                                probed_features, _ = probe_features_for_target(
                                    loader=loader,
                                    symbols=symbols_to_load,
                                    target=target_column,
                                    preflight_features=preflight_features,
                                    top_n=probe_top_n,
                                    probe_rows=probe_rows,
                                )
                                logger.info(f"   üî¨ Probe: {len(preflight_features)} ‚Üí {len(probed_features)} important columns")
                                columns_to_load = probed_features
                            else:
                                columns_to_load = preflight_features

                            # Include target and metadata columns
                            metadata_cols = ['ts', 'symbol', 'date', 'time', target_column]
                            for col in metadata_cols:
                                if col not in columns_to_load:
                                    columns_to_load.append(col)
                            columns_to_load = sorted(set(columns_to_load))

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Lazy loading setup failed: {e}. Falling back to full load.")
                        columns_to_load = None

            except Exception as e:
                logger.debug(f"Lazy loading config not available: {e}")

        logger.info(f"Loading data for {len(symbols_to_load)} symbol(s) (max {self.max_rows_per_symbol} rows per symbol)...")
        if columns_to_load:
            logger.info(f"   üìä Column projection: loading {len(columns_to_load)} columns (vs all)")
        if isinstance(self.view, str) and self.view == "LOSO":
            logger.info(f"  LOSO: Training on {len(symbols_to_load)} symbols, validating on {validation_symbol}")

        mtf_data = load_mtf_data_for_ranking(
            self.data_dir,
            symbols_to_load,
            max_rows_per_symbol=self.max_rows_per_symbol,
            columns=columns_to_load
        )
        
        if not mtf_data:
            logger.error(f"No data loaded for any symbols")
            return None, None, None, None, None, None, None, None
        
        # FIX: Check if CROSS_SECTIONAL mode has insufficient symbols and handle gracefully
        n_symbols_loaded = len(mtf_data)
        MIN_SYMBOLS_FOR_CROSS_SECTIONAL = 3
        
        if self.view == View.CROSS_SECTIONAL and n_symbols_loaded < MIN_SYMBOLS_FOR_CROSS_SECTIONAL:
            loaded_symbols_list = list(mtf_data.keys())
            if n_symbols_loaded == 1:
                # Single symbol: automatically switch to SYMBOL_SPECIFIC mode
                logger.warning(
                    f"‚ö†Ô∏è  CROSS_SECTIONAL mode requires >= {MIN_SYMBOLS_FOR_CROSS_SECTIONAL} symbols, "
                    f"but only {n_symbols_loaded} loaded ({loaded_symbols_list}). "
                    f"Automatically switching to SYMBOL_SPECIFIC mode for symbol {loaded_symbols_list[0]}."
                )
                self.view = View.SYMBOL_SPECIFIC
                self.symbol = loaded_symbols_list[0]
            else:
                # 2 symbols: still insufficient, but log warning and proceed with allow_single_symbol=True
                logger.warning(
                    f"‚ö†Ô∏è  CROSS_SECTIONAL mode recommended >= {MIN_SYMBOLS_FOR_CROSS_SECTIONAL} symbols, "
                    f"but only {n_symbols_loaded} loaded ({loaded_symbols_list}). "
                    f"Proceeding with reduced cross-sectional width (may be less robust)."
                )
        
        # Get sample dataframe for interval detection and feature filtering
        # DETERMINISTIC: Sort symbols before picking sample to ensure consistent column discovery
        sample_symbol = sorted(mtf_data.keys())[0] if mtf_data else None
        if sample_symbol:
            sample_df = mtf_data[sample_symbol]
            all_columns = sample_df.columns.tolist()
        else:
            all_columns = []
        
        # TARGET-CONDITIONAL EXCLUSIONS: Generate per-target exclusion list
        # This implements "Target-Conditional Feature Selection" - tailoring features to target physics
        target_conditional_exclusions = []
        exclusion_metadata = {}
        target_exclusion_dir = None
        
        if self.output_dir and target:
            # Determine base output directory (handle both old and new call patterns)
            base_output_dir = Path(self.output_dir)
            if base_output_dir.name in ["target_rankings", "feature_selections"]:
                base_output_dir = base_output_dir.parent
            
            # FIX: Check if we're already inside REPRODUCIBILITY structure at target level
            # If output_dir is already REPRODUCIBILITY/FEATURE_SELECTION/CROSS_SECTIONAL/{target}/,
            # use it directly instead of reconstructing the path
            from TRAINING.orchestration.utils.target_first_paths import normalize_target_name
            target_clean = normalize_target_name(target)
            
            # Check if we're already at the target level inside REPRODUCIBILITY
            is_already_in_repro = "REPRODUCIBILITY" in str(base_output_dir)
            is_at_target_level = (
                base_output_dir.name == target_clean or
                (base_output_dir.parent.name in [View.CROSS_SECTIONAL.value, View.SYMBOL_SPECIFIC.value] and
                 base_output_dir.parent.parent.name in ["FEATURE_SELECTION", "TARGET_RANKING"])
            )
            
            if is_already_in_repro and is_at_target_level:
                # Already at target level - use it directly
                target_exclusion_dir = base_output_dir / "feature_exclusions"
            else:
                # Need to construct path - walk up to find run level if we're inside REPRODUCIBILITY
                if is_already_in_repro:
                    # Walk up until we find the run directory (parent of REPRODUCIBILITY)
                    current = base_output_dir
                    while current.name != "REPRODUCIBILITY" and current.parent != current:
                        current = current.parent
                    if current.name == "REPRODUCIBILITY":
                        base_output_dir = current.parent
                    # If we couldn't find it, log warning and use original
                    if "REPRODUCIBILITY" in str(base_output_dir):
                        logger.warning(f"Could not resolve REPRODUCIBILITY base from {self.output_dir}, using as-is")
                
                # Save to target-first structure with view scoping
                # (targets/<target>/reproducibility/<VIEW>/[symbol=...]/[universe=...]/feature_exclusions/)
                from TRAINING.orchestration.utils.target_first_paths import (
                    ensure_scoped_artifact_dir, ensure_target_structure
                )
                ensure_target_structure(base_output_dir, target_clean)
                # Determine stage based on job_type - use Stage enum (already imported globally)
                stage = Stage.FEATURE_SELECTION if self.job_type == "rank_features" else Stage.TARGET_RANKING
                target_exclusion_dir = ensure_scoped_artifact_dir(
                    base_output_dir, target_clean, "feature_exclusions",
                    view=self.view, symbol=self.symbol, universe_sig=self.universe_sig,
                    stage=stage,  # Explicit stage for proper scoping
                    attempt_id=0  # Default to attempt_0 (attempt_id not available in this context)
                )
            
            target_exclusion_dir.mkdir(parents=True, exist_ok=True)
            
            # Try to load existing exclusion list first (check target-first structure with view scoping)
            existing_exclusions = load_target_exclusion_list(target, target_exclusion_dir)
            if existing_exclusions is None:
                # Fallback to old target-first location (unscoped by view)
                from TRAINING.orchestration.utils.target_first_paths import get_target_reproducibility_dir
                legacy_target_dir = get_target_reproducibility_dir(base_output_dir, target_clean) / "feature_exclusions"
                existing_exclusions = load_target_exclusion_list(target, legacy_target_dir)
            if existing_exclusions is None:
                # Fallback to legacy REPRODUCIBILITY location
                if self.job_type == "rank_targets":
                    # Convert view enum to string for path construction
                    view_str = str(self.view) if isinstance(self.view, View) else self.view
                    legacy_exclusion_dir = base_output_dir / "REPRODUCIBILITY" / "TARGET_RANKING" / view_str / target_clean / "feature_exclusions"
                elif self.job_type == "rank_features":
                    # Convert view enum to string for path construction
                    view_subdir = str(self.view) if isinstance(self.view, View) else (self.view if self.view else View.CROSS_SECTIONAL.value)
                    legacy_exclusion_dir = base_output_dir / "REPRODUCIBILITY" / "FEATURE_SELECTION" / view_subdir / target_clean / "feature_exclusions"
                    if self.view == View.SYMBOL_SPECIFIC and self.symbol:
                        legacy_exclusion_dir = legacy_exclusion_dir.parent.parent / f"symbol={self.symbol}" / "feature_exclusions"
                else:
                    legacy_exclusion_dir = base_output_dir / "feature_exclusions"
                existing_exclusions = load_target_exclusion_list(target, legacy_exclusion_dir)
            if existing_exclusions is not None:
                target_conditional_exclusions = existing_exclusions
                logger.info(
                    f"üìã Loaded existing target-conditional exclusions for {target}: "
                    f"{len(target_conditional_exclusions)} features "
                    f"(from {target_exclusion_dir})"
                )
            else:
                # Generate new exclusion list
                try:
                    from TRAINING.common.feature_registry import get_registry
                    registry = get_registry()
                except Exception:
                    registry = None
                
                # Detect interval for lookback calculation
                temp_interval = detect_interval_from_dataframe(
                    sample_df, 
                    explicit_interval=self.explicit_interval,
                    experiment_config=self.experiment_config
                )
                
                target_conditional_exclusions, exclusion_metadata = generate_target_exclusion_list(
                    target=target,
                    all_features=all_columns,
                    interval_minutes=temp_interval,
                    output_dir=target_exclusion_dir,
                    registry=registry
                )
                
                # Also save to legacy location for backward compatibility
                if target_conditional_exclusions and 'legacy_exclusion_dir' in locals():
                    try:
                        import shutil
                        from TRAINING.orchestration.utils.target_first_paths import normalize_target_name
                        safe_target = normalize_target_name(target)
                        exclusion_file = target_exclusion_dir / f"{safe_target}_exclusions.yaml"
                        legacy_exclusion_file = legacy_exclusion_dir / f"{safe_target}_exclusions.yaml"
                        if exclusion_file.exists():
                            shutil.copy2(exclusion_file, legacy_exclusion_file)
                            logger.debug(f"Saved exclusion file to legacy location: {legacy_exclusion_file}")
                    except Exception as e:
                        logger.debug(f"Failed to copy exclusion file to legacy location: {e}")
                
                if target_conditional_exclusions:
                    logger.info(
                        f"üìã Generated target-conditional exclusions for {target}: "
                        f"{len(target_conditional_exclusions)} features excluded "
                        f"(horizon={exclusion_metadata.get('target_horizon_minutes', 'unknown')}m, "
                        f"semantics={exclusion_metadata.get('target_semantics', {})})"
                    )
        else:
            logger.debug("No output_dir or target provided - skipping target-conditional exclusions")
        
        # Detect data interval
        detected_interval = detect_interval_from_dataframe(
            sample_df,
            timestamp_column='ts',
            default=5,
            explicit_interval=self.explicit_interval,
            experiment_config=self.experiment_config
        )
        
        # Extract target horizon for error messages and resolved config
        leakage_config = _load_leakage_config()
        target_horizon_minutes = _extract_horizon(target_column, leakage_config) if target_column else None
        
        # Apply target-conditional exclusions BEFORE global filtering
        columns_after_target_exclusions = [c for c in all_columns if c not in target_conditional_exclusions]
        
        if target_conditional_exclusions:
            logger.info(
                f"  üéØ Target-conditional exclusions: Removed {len(target_conditional_exclusions)} features "
                f"({len(columns_after_target_exclusions)} remaining before global filtering)"
            )
        
        # Resolve registry overlay directory (for feature selection to consume target ranking patches)
        registry_overlay_dir = None
        if self.output_dir and use_strict_registry:  # Only resolve for feature selection (strict mode)
            try:
                from TRAINING.orchestration.utils.target_first_paths import run_root
                from TRAINING.ranking.utils.registry_overlay_resolver import resolve_registry_overlay_dir_for_feature_selection
                
                run_output_root = run_root(self.output_dir)
                overlay_resolution = resolve_registry_overlay_dir_for_feature_selection(
                    run_output_root=run_output_root,
                    experiment_config=self.experiment_config,
                    target_column=target_column,
                    current_bar_minutes=detected_interval
                )
                registry_overlay_dir = overlay_resolution.overlay_dir
                
                if overlay_resolution.overlay_kind == "patch":
                    logger.info(
                        f"üìã Using registry patch for {target_column}: {overlay_resolution.patch_file.name} "
                        f"(signature: {overlay_resolution.overlay_signature[:16] if overlay_resolution.overlay_signature else 'none'}...)"
                    )
                elif overlay_resolution.overlay_kind == "config":
                    logger.debug(f"Using config registry overlay for {target_column}: {overlay_resolution.overlay_dir}")
            except Exception as e:
                logger.debug(f"Could not resolve registry overlay for {target_column}: {e}")
        
        # Apply leakage filtering if feature_names not provided
        if feature_names is None:
            safe_columns = filter_features_for_target(
                columns_after_target_exclusions,  # Use pre-filtered columns
                target_column,
                verbose=True,
                use_registry=True,
                data_interval_minutes=detected_interval,
                for_ranking=not use_strict_registry,  # Use strict mode for feature selection, permissive for target ranking
                registry_overlay_dir=registry_overlay_dir  # Pass resolved overlay directory
            )
            feature_names = safe_columns
        
        excluded_count = len(all_columns) - len(feature_names) - 1  # -1 for target itself
        features_safe = len(feature_names)
        logger.debug(f"Filtered out {excluded_count} potentially leaking features (kept {features_safe} safe features)")
        
        # Apply runtime quarantine (dominance quarantine confirmed features)
        if self.output_dir:
            try:
                from TRAINING.ranking.utils.dominance_quarantine import load_confirmed_quarantine
                runtime_quarantine = load_confirmed_quarantine(
                    output_dir=self.output_dir,
                    target=target_column,
                    view=self.view,
                    symbol=self.symbol
                )
                if runtime_quarantine:
                    feature_names = [f for f in feature_names if f not in runtime_quarantine]
                    logger.info(f"üîí Applied runtime quarantine: Removed {len(runtime_quarantine)} confirmed leaky features ({len(feature_names)} remaining)")
            except Exception as e:
                logger.debug(f"Could not load runtime quarantine: {e}")
        
        # Check if we have enough features to train
        try:
            from CONFIG.config_loader import get_cfg
            from CONFIG.config_loader import get_safety_config
            safety_cfg = get_safety_config()
            safety_section = safety_cfg.get('safety', {})
            leakage_cfg = safety_section.get('leakage_detection', {})
            ranking_cfg = leakage_cfg.get('ranking', {})
            MIN_FEATURES_REQUIRED = int(ranking_cfg.get('min_features_required', 2))
        except Exception:
            MIN_FEATURES_REQUIRED = 2
        
        if len(feature_names) < MIN_FEATURES_REQUIRED:
            target_horizon_bars = None
            if target_horizon_minutes is not None and detected_interval > 0:
                target_horizon_bars = int(target_horizon_minutes // detected_interval)
            # Always log both minutes and bars for clarity
            if target_horizon_minutes is not None and target_horizon_bars is not None:
                horizon_info = f"horizon_minutes={target_horizon_minutes:.1f}m, horizon_bars={target_horizon_bars} bars @ interval={detected_interval:.1f}m"
            elif target_horizon_bars is not None:
                horizon_info = f"horizon_bars={target_horizon_bars} bars @ interval={detected_interval:.1f}m"
            else:
                horizon_info = "this horizon"
            logger.error(
                f"‚ùå INSUFFICIENT FEATURES: Only {len(feature_names)} features remain after filtering "
                f"(minimum required: {MIN_FEATURES_REQUIRED}). "
                f"This target may not be predictable with current feature set."
            )
            return None, None, None, None, None, None, detected_interval, None
        
        # Create resolved_config EARLY (before data prep) to get feature_time_meta_map for alignment
        # We'll update feature counts after data prep
        n_symbols_available = len(mtf_data)
        resolved_config = create_resolved_config(
            requested_min_cs=self.min_cs if self.view != View.SYMBOL_SPECIFIC else 1,
            n_symbols_available=n_symbols_available,
            max_cs_samples=self.max_cs_samples,
            interval_minutes=detected_interval,
            horizon_minutes=target_horizon_minutes,
            feature_lookback_max_minutes=None,  # Will be computed from feature_names
            purge_buffer_bars=5,
            default_purge_minutes=None,  # Loads from safety_config.yaml
            features_safe=features_safe,
            features_dropped_nan=0,  # Will be updated after data prep
            features_final=features_safe,  # Will be updated after data prep
            view=str(self.view) if isinstance(self.view, View) else self.view,  # Convert enum to string for resolved_config
            symbol=self.symbol,
            feature_names=feature_names,  # Pass feature names for feature_time_meta_map building
            recompute_lookback=True,  # CRITICAL: Compute feature lookback to auto-adjust purge
            experiment_config=getattr(self, 'experiment_config', None)  # NEW: Pass experiment_config if available
        )
        
        # Prepare data based on view (with alignment args from resolved_config)
        # NOTE: view may have been changed to SYMBOL_SPECIFIC above if only 1 symbol loaded
        # Pass requested_view and output_dir for view resolution and persistence
        requested_view_for_prep = self.view  # Use current view as requested_view
        if self.view == View.SYMBOL_SPECIFIC:
            # For symbol-specific, prepare single-symbol time series data
            X, y, feature_names_out, symbols_array, time_vals, resolved_data_config = prepare_cross_sectional_data_for_ranking(
                mtf_data, target_column, min_cs=1, max_cs_samples=self.max_cs_samples, feature_names=feature_names,
                feature_time_meta_map=resolved_config.feature_time_meta_map,  # NEW: Pass from resolved_config
                base_interval_minutes=resolved_config.base_interval_minutes,  # NEW: Pass from resolved_config
                allow_single_symbol=True,  # SYMBOL_SPECIFIC always allows single symbol
                requested_view=requested_view_for_prep,
                output_dir=self.output_dir
            )
        elif self.view == "LOSO":
            # LOSO: prepare training data (all symbols except validation symbol)
            X, y, feature_names_out, symbols_array, time_vals, resolved_data_config = prepare_cross_sectional_data_for_ranking(
                mtf_data, target_column, min_cs=self.min_cs, max_cs_samples=self.max_cs_samples, feature_names=feature_names,
                feature_time_meta_map=resolved_config.feature_time_meta_map,  # NEW: Pass from resolved_config
                base_interval_minutes=resolved_config.base_interval_minutes,  # NEW: Pass from resolved_config
                requested_view=requested_view_for_prep,
                output_dir=self.output_dir
            )
            # TODO: Handle validation symbol separately for LOSO
            logger.warning("LOSO view: Using combined data for now (LOSO-specific CV splitter not yet implemented)")
        else:
            # CROSS_SECTIONAL: standard pooled data
            # FIX: Pass allow_single_symbol=True if we have < 3 symbols (graceful degradation)
            # This prevents ValueError when only 1-2 symbols are loaded
            # Note: If view was changed to SYMBOL_SPECIFIC above, we won't reach this block
            allow_single_symbol = (n_symbols_loaded < 3)  # Use literal to avoid scope issues
            X, y, feature_names_out, symbols_array, time_vals, resolved_data_config = prepare_cross_sectional_data_for_ranking(
                mtf_data, target_column, min_cs=self.min_cs, max_cs_samples=self.max_cs_samples, feature_names=feature_names,
                feature_time_meta_map=resolved_config.feature_time_meta_map,  # NEW: Pass from resolved_config
                base_interval_minutes=resolved_config.base_interval_minutes,  # NEW: Pass from resolved_config
                allow_single_symbol=allow_single_symbol,  # FIX: Allow graceful degradation for < 3 symbols
                requested_view=requested_view_for_prep,  # FIX: Pass for view caching/persistence
                output_dir=self.output_dir  # FIX: Pass for view caching/persistence
            )
        
        # Update feature counts after data preparation
        features_dropped_nan = 0
        features_final = features_safe
        if feature_names_out is not None:
            features_final = len(feature_names_out)
            features_dropped_nan = features_safe - features_final
        
        # Update resolved_config with final feature counts (using dataclasses.replace for frozen dataclass)
        if hasattr(resolved_config, '__dict__'):
            # ResolvedConfig is not frozen, can update directly
            resolved_config.features_dropped_nan = features_dropped_nan
            resolved_config.features_final = features_final
        else:
            # If frozen, would need dataclasses.replace, but ResolvedConfig is not frozen
            resolved_config.features_dropped_nan = features_dropped_nan
            resolved_config.features_final = features_final
        
        # PERFORMANCE AUDIT: Track build_panel completion
        build_duration = time.time() - build_start_time
        if auditor and auditor.enabled:
            rows = X.shape[0] if X is not None else None
            cols = len(feature_names_out) if feature_names_out else None
            view_str = self.view.value if hasattr(self.view, 'value') else str(self.view)
            auditor.track_call(
                func_name='RankingHarness.build_panel',
                duration=build_duration,
                rows=rows,
                cols=cols,
                stage=self.job_type,  # 'rank_targets' or 'rank_features'
                cache_hit=False,  # build_panel doesn't use cache currently
                input_fingerprint=fingerprint,
                target=self.target_column,
                symbol=self.symbol,
                view=view_str,
            )
        
        return X, y, feature_names_out, symbols_array, time_vals, mtf_data, detected_interval, resolved_config, resolved_data_config
    
    def split_policy(
        self,
        time_vals: np.ndarray,
        groups: Optional[np.ndarray] = None,
        horizon_minutes: Optional[float] = None,
        data_interval_minutes: float = 5.0
    ) -> Any:
        """
        Create split policy (PurgedTimeSeriesSplit) with time-based purging.
        
        This is the SAME split policy used by target ranking to ensure
        identical evaluation contracts.
        
        Args:
            time_vals: Timestamps for each sample
            groups: Optional grouping array (for panel data)
            horizon_minutes: Target horizon in minutes (for purge calculation)
            data_interval_minutes: Data bar interval in minutes
        
        Returns:
            PurgedTimeSeriesSplit instance
        
        Raises:
            ValueError: If data span is insufficient for the required purge/embargo
        """
        from TRAINING.ranking.utils.purged_time_series_split import PurgedTimeSeriesSplit
        from TRAINING.ranking.utils.resolved_config import derive_purge_embargo
        
        # Get CV config
        cv_config = self.multi_model_config.get('cross_validation', {}) if self.multi_model_config else {}
        try:
            from CONFIG.config_loader import get_cfg
            folds = int(get_cfg("training.folds", default=cv_config.get('folds', 3), config_name="intelligent_training_config"))
        except Exception:
            folds = cv_config.get('folds', 3)
        
        # Derive purge and embargo from horizon
        if horizon_minutes is not None:
            purge_minutes, embargo_minutes = derive_purge_embargo(
                horizon_minutes=horizon_minutes,
                interval_minutes=data_interval_minutes,
                feature_lookback_max_minutes=None,  # Will be computed from features if needed
                purge_buffer_bars=5,
                default_purge_minutes=None  # Loads from safety_config.yaml
            )

            # CRITICAL: Validate that data span is sufficient for purge/embargo
            # Rule of thumb: need at least (purge + embargo) * 2 * folds worth of data
            # This ensures each fold has enough training data after purging
            if time_vals is not None and len(time_vals) > 1:
                time_vals_sorted = np.sort(time_vals)
                data_span_minutes = float((time_vals_sorted[-1] - time_vals_sorted[0]) / (60 * 1e9))  # Convert ns to minutes
                required_span_minutes = (purge_minutes + embargo_minutes) * 2 * folds

                if data_span_minutes < required_span_minutes:
                    raise ValueError(
                        f"Insufficient data span for long-horizon target (horizon={horizon_minutes:.1f}m). "
                        f"Data span: {data_span_minutes:.1f}m, Required: {required_span_minutes:.1f}m "
                        f"(purge={purge_minutes:.1f}m + embargo={embargo_minutes:.1f}m) * 2 * {folds} folds. "
                        f"Falling back to per-symbol processing (expected for long-horizon targets with limited data). "
                        f"To use shared harness: 1) Increase max_rows_per_symbol, 2) Reduce horizon, or 3) Skip this target."
                    )
        else:
            # Fallback: use default purge (60m = 12 bars + buffer)
            purge_minutes = 60.0

        # Create time-based purged splitter (REQUIRED for panel data)
        # Uses purge_overlap_minutes (simpler API) instead of purge_overlap_time
        cv_splitter = PurgedTimeSeriesSplit(
            n_splits=folds,
            purge_overlap_minutes=purge_minutes,
            time_column_values=time_vals
        )
        
        return cv_splitter
    
    def run_importance_producers(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: List[str],
        time_vals: Optional[np.ndarray] = None,
        task_type: Any = None,
        resolved_config: Optional[Any] = None
    ) -> Tuple[Dict[str, Dict[str, float]], Dict[str, float], float,
               Dict[str, List[Tuple[str, float]]], Dict[str, Dict[str, float]],
               List[Dict[str, Any]], set]:
        """
        Run importance producers (models) using the SAME evaluation harness.
        
        This calls train_and_evaluate_models which uses:
        - PurgedTimeSeriesSplit with time-based purging
        - Same scoring functions
        - Same leakage-safe imputation
        - Same fold timestamp tracking
        
        Args:
            X: Feature matrix
            y: Target array
            feature_names: List of feature names
            time_vals: Timestamps for each sample
            task_type: TaskType enum
            resolved_config: Optional ResolvedConfig with purge/embargo
        
        Returns:
            Tuple of (model_metrics, model_scores, mean_importance,
                     all_suspicious_features, all_feature_importances, fold_timestamps, perfect_correlation_models)
        """
        from TRAINING.ranking.predictability.model_evaluation import train_and_evaluate_models
        from TRAINING.ranking.predictability.scoring import TaskType
        
        # Infer task type if not provided
        if task_type is None:
            y_sample = pd.Series(y).dropna()
            task_type = TaskType.from_target_column(self.target_column, y_sample.to_numpy())
        
        # Filter families by task type compatibility BEFORE passing to train_and_evaluate_models
        # This prevents "MissingFromHarness" errors for families that are correctly filtered
        # (e.g., lasso is regression-only, so it's filtered for binary classification tasks)
        from TRAINING.training_strategies.utils import is_family_compatible
        compatible_families = []
        skipped_families = []
        for family in self.model_families:
            compatible, skip_reason = is_family_compatible(family, task_type)
            if compatible:
                compatible_families.append(family)
            else:
                skipped_families.append((family, skip_reason))
                logger.info(f"‚è≠Ô∏è Skipping {family} for {self.job_type}: {skip_reason} (task={task_type})")
        if skipped_families:
            logger.info(f"üìã Filtered {len(skipped_families)} incompatible families for task={task_type}")
        
        # Use only compatible families (train_and_evaluate_models will filter again, but this prevents confusion)
        filtered_model_families = compatible_families
        
        # Detect data interval from time_vals if available
        data_interval_minutes = 5.0  # Default
        if time_vals is not None and len(time_vals) > 1:
            try:
                time_series = pd.Series(time_vals) if not isinstance(time_vals, pd.Series) else time_vals
                if pd.api.types.is_datetime64_any_dtype(time_series):
                    unique_times = time_series.unique()
                    unique_times_sorted = pd.Series(unique_times).sort_values()
                    time_diffs = unique_times_sorted.diff().dropna()
                    if isinstance(time_diffs, pd.TimedeltaIndex) and len(time_diffs) > 0:
                        median_diff_minutes = abs(time_diffs.median().total_seconds()) / 60.0
                        # Round to common intervals
                        common_intervals = [1, 5, 15, 30, 60]
                        detected_interval = min(common_intervals, key=lambda x: abs(x - median_diff_minutes))
                        data_interval_minutes = detected_interval
            except Exception:
                pass
        
        # Call the SAME train_and_evaluate_models function used by target ranking
        # Note: train_and_evaluate_models will filter again internally, but we've already filtered here
        # to provide clear logging and prevent "MissingFromHarness" confusion
        results = train_and_evaluate_models(
            X=X,
            y=y,
            feature_names=feature_names,
            task_type=task_type,
            model_families=filtered_model_families,  # Use filtered list
            multi_model_config=self.multi_model_config,
            target_column=self.target_column,
            data_interval_minutes=data_interval_minutes,
            time_vals=time_vals,
            explicit_interval=self.explicit_interval,
            experiment_config=self.experiment_config,
            output_dir=self.output_dir,
            resolved_config=resolved_config
        )
        
        return results
    
    def create_run_context(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: List[str],
        symbols_array: np.ndarray,
        time_vals: np.ndarray,
        cv_splitter: Any,
        horizon_minutes: Optional[float] = None,
        purge_minutes: Optional[float] = None,
        embargo_minutes: Optional[float] = None,
        data_interval_minutes: Optional[float] = None,
        min_cs: Optional[int] = None,  # FIX: Add min_cs for diff telemetry
        max_cs_samples: Optional[int] = None  # FIX: Add max_cs_samples for diff telemetry
    ) -> Any:
        """
        Create RunContext for reproducibility tracking.
        
        This ensures both target ranking and feature selection use the SAME
        RunContext fields for consistent telemetry.
        
        Args:
            X: Feature matrix
            y: Target array
            feature_names: List of feature names
            symbols_array: Symbol array
            time_vals: Timestamps
            cv_splitter: CV splitter instance
            horizon_minutes: Target horizon
            purge_minutes: Purge minutes
            embargo_minutes: Embargo minutes
            data_interval_minutes: Data interval
        
        Returns:
            RunContext instance
        """
        from TRAINING.orchestration.utils.run_context import RunContext
        
        # Extract folds from splitter if available (required for COHORT_AWARE mode)
        folds = None
        if cv_splitter is not None:
            # Try to get n_splits from splitter (PurgedTimeSeriesSplit has n_splits attribute)
            if hasattr(cv_splitter, 'n_splits'):
                folds = cv_splitter.n_splits
            elif hasattr(cv_splitter, 'get_n_splits'):
                try:
                    folds = cv_splitter.get_n_splits()
                except Exception:
                    pass
        
        ctx = RunContext(
            stage=Stage.FEATURE_SELECTION if self.job_type == "rank_features" else Stage.TARGET_RANKING,
            target=self.target_column,
            target_column=self.target_column,
            X=X,
            y=y,
            feature_names=feature_names,
            symbols=symbols_array.tolist() if isinstance(symbols_array, np.ndarray) else symbols_array,
            time_vals=time_vals,
            horizon_minutes=horizon_minutes,
            purge_minutes=purge_minutes,
            embargo_minutes=embargo_minutes,
            data_interval_minutes=data_interval_minutes,
            cv_splitter=cv_splitter,
            folds=folds,  # FIX: Extract and pass folds for COHORT_AWARE mode
            view=self.view,
            symbol=self.symbol,
            min_cs=min_cs if min_cs is not None else self.min_cs,  # FIX: Populate min_cs for diff telemetry
            max_cs_samples=max_cs_samples if max_cs_samples is not None else self.max_cs_samples  # FIX: Populate max_cs_samples for diff telemetry
        )
        
        return ctx
    
    def sanitize_and_canonicalize_dtypes(
        self,
        X: np.ndarray,
        feature_names: List[str]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        Sanitize data and enforce numeric dtypes.
        
        This is done ONCE upstream and reused everywhere to prevent
        dtype issues (e.g., CatBoost object column errors).
        
        Args:
            X: Feature matrix (can be DataFrame or array)
            feature_names: List of feature names
        
        Returns:
            Tuple of (X_sanitized, feature_names_sanitized)
        """
        # Convert to DataFrame if needed
        if isinstance(X, np.ndarray):
            X_df = pd.DataFrame(X, columns=feature_names)
        else:
            X_df = X.copy()
        
        # FIX: Hard guardrail - enforce numeric dtypes BEFORE any model training
        # This prevents CatBoost from treating numeric columns as text/categorical
        # CatBoost "categoricalizing" numeric features causes fake performance (perfect scores)
        # NOTE: np and pd are imported at module scope - do not import locally to avoid UnboundLocalError
        
        # Step 1: Try to convert object columns to numeric (don't drop immediately)
        object_cols = X_df.select_dtypes(include=['object', 'string', 'category']).columns.tolist()
        if object_cols:
            logger.warning(f"Found {len(object_cols)} object/string/category columns: {object_cols[:10]}")
            logger.warning("Attempting to convert to numeric (coercing errors to NaN)")
            for col in object_cols:
                try:
                    X_df[col] = pd.to_numeric(X_df[col], errors='coerce').astype('float32')
                    logger.debug(f"Converted object column {col} to float32")
                except Exception as e:
                    logger.warning(f"Failed to convert {col} to numeric: {e}, will drop")
        
        # Step 2: Ensure all remaining columns are numeric (hard-fail if not)
        still_bad = [c for c in X_df.columns if not pd.api.types.is_numeric_dtype(X_df[c])]
        if still_bad:
            logger.error(f"Non-numeric columns remain after conversion: {still_bad[:10]}")
            logger.error("Dropping non-numeric columns to prevent dtype errors")
            X_df = X_df.drop(columns=still_bad)
            feature_names = [f for f in feature_names if f not in still_bad]
        
        # Step 3: Hard-cast all numeric columns to float32 (prevents object dtype from NaN/mixed types)
        # This is critical - CatBoost can interpret float64 with NaN as object dtype
        for col in X_df.columns:
            if pd.api.types.is_numeric_dtype(X_df[col]):
                X_df[col] = X_df[col].astype('float32')
            else:
                # Should not happen after Step 2, but defensive check
                logger.error(f"Column {col} is still not numeric (dtype={X_df[col].dtype}), dropping")
                X_df = X_df.drop(columns=[col])
                feature_names = [f for f in feature_names if f != col]
        
        # Step 4: Final verification - fail fast if any non-numeric remain
        final_bad = [c for c in X_df.columns if not np.issubdtype(X_df[c].dtype, np.number)]
        if final_bad:
            raise TypeError(f"CRITICAL: Non-numeric columns remain after all conversions: {final_bad[:10]}. "
                          f"This will cause CatBoost to treat them as text/categorical and fake performance.")
        
        # FIX: Replace inf/-inf with nan before fail-fast (prevents phantom issues)
        # Some models (e.g., Ridge) may fail on inf values
        X_df = X_df.replace([np.inf, -np.inf], np.nan)
        # Drop columns that are all nan/inf
        X_df = X_df.dropna(axis=1, how='all')
        feature_names = [f for f in feature_names if f in X_df.columns]
        
        # Convert back to numpy array
        X_sanitized = X_df.values
        
        return X_sanitized, feature_names
    
    def apply_cleaning_and_audit_checks(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: List[str],
        target_column: str,
        resolved_config: Any,
        detected_interval: float,
        task_type: Any = None
    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[List[str]], Optional[Any], bool]:
        """
        Apply all cleaning and audit checks from target ranking.
        
        This includes:
        - Duplicate column name checks
        - Pre-training leak scan (find_near_copy_features)
        - Target validation
        - Degenerate target checks
        - Class imbalance checks
        - Final gatekeeper (ghost buster)
        
        Args:
            X: Feature matrix
            y: Target array
            feature_names: List of feature names
            target_column: Target column name
            resolved_config: ResolvedConfig object
            detected_interval: Data interval in minutes
            task_type: Optional TaskType (will be inferred if None)
        
        Returns:
            Tuple of (X_cleaned, y_cleaned, feature_names_cleaned, resolved_config_updated, success)
            If success is False, X_cleaned will be None
        """
        from TRAINING.ranking.predictability.leakage_detection import find_near_copy_features
        from TRAINING.ranking.predictability.model_evaluation import _enforce_final_safety_gate, validate_target
        from TRAINING.ranking.predictability.scoring import TaskType
        from TRAINING.ranking.utils.cross_sectional_data import _log_feature_set
        from TRAINING.ranking.utils.resolved_config import compute_feature_lookback_max
        
        # Infer task type if not provided
        if task_type is None:
            y_sample = pd.Series(y).dropna()
            task_type = TaskType.from_target_column(target_column, y_sample.to_numpy())
        
        # Check for duplicate column names
        if len(feature_names) != len(set(feature_names)):
            # DETERMINISM: Use sorted_unique for deterministic iteration order
            duplicates = [name for name in sorted_unique(feature_names) if feature_names.count(name) > 1]
            logger.error(f"  üö® DUPLICATE COLUMN NAMES DETECTED: {duplicates}")
            raise ValueError(f"Duplicate feature names detected: {duplicates}")
        
        # PRE-TRAINING LEAK SCAN: Detect and remove near-copy features
        logger.info("üîç Pre-training leak scan: Checking for near-copy features...")
        feature_names_before_leak_scan = feature_names.copy()
        
        X_df = pd.DataFrame(X, columns=feature_names)
        y_series = pd.Series(y)
        leaky_features = find_near_copy_features(X_df, y_series, task_type)
        
        if leaky_features:
            logger.error(
                f"  ‚ùå CRITICAL: Found {len(leaky_features)} leaky features that are near-copies of target: {leaky_features}"
            )
            logger.error(
                f"  Removing leaky features and continuing with {X.shape[1] - len(leaky_features)} features..."
            )
            
            # Remove leaky features
            leaky_indices = [i for i, name in enumerate(feature_names) if name in leaky_features]
            X = np.delete(X, leaky_indices, axis=1)
            feature_names = [name for name in feature_names if name not in leaky_features]
            
            logger.info(f"  After leak removal: {X.shape[1]} features remaining")
            _log_feature_set("AFTER_LEAK_REMOVAL", feature_names, previous_names=feature_names_before_leak_scan, logger_instance=logger)
            
            # Check if we removed too many features
            try:
                from CONFIG.config_loader import get_safety_config
                safety_cfg = get_safety_config()
                safety_section = safety_cfg.get('safety', {})
                leakage_cfg = safety_section.get('leakage_detection', {})
                ranking_cfg = leakage_cfg.get('ranking', {})
                MIN_FEATURES_AFTER_LEAK_REMOVAL = int(ranking_cfg.get('min_features_after_leak_removal', 2))
            except Exception:
                MIN_FEATURES_AFTER_LEAK_REMOVAL = 2
            
            if X.shape[1] < MIN_FEATURES_AFTER_LEAK_REMOVAL:
                logger.error(
                    f"  ‚ùå Too few features remaining after leak removal ({X.shape[1]}). "
                    f"Marking as LEAKAGE_DETECTED."
                )
                return None, None, None, None, False
        else:
            logger.info("  ‚úÖ No obvious leaky features detected in pre-training scan")
            _log_feature_set("AFTER_LEAK_REMOVAL", feature_names, previous_names=feature_names_before_leak_scan, logger_instance=logger)
        
        # Early exit if too few features
        try:
            from CONFIG.config_loader import get_safety_config
            from CONFIG.config_loader import get_cfg
            MIN_FEATURES_FOR_MODEL = int(get_cfg(
                "thresholds.min_features_for_model",
                default=3,
                config_name="feature_selection_config"
            ))
        except Exception:
            MIN_FEATURES_FOR_MODEL = 3  # Fallback if config system unavailable (defensive boundary)
        
        if X.shape[1] < MIN_FEATURES_FOR_MODEL:
            logger.warning(
                f"Too few features ({X.shape[1]}) after filtering (minimum: {MIN_FEATURES_FOR_MODEL}); "
                f"marking as degenerate and skipping."
            )
            return None, None, None, None, False
        
        # Validate target
        is_valid, error_msg = validate_target(y, task_type=task_type)
        if not is_valid:
            logger.warning(f"Skipping: {error_msg}")
            return None, None, None, None, False
        
        # Check if target is degenerate
        unique_vals = np.unique(y[~np.isnan(y)])
        if len(unique_vals) < 2:
            logger.warning(f"Skipping: Target has only {len(unique_vals)} unique value(s)")
            return None, None, None, None, False
        
        # For classification, check if classes are too imbalanced for CV
        if len(unique_vals) <= 10:  # Likely classification
            class_counts = np.bincount(y[~np.isnan(y)].astype(int))
            min_class_count = class_counts[class_counts > 0].min()
            if min_class_count < 2:
                logger.warning(f"Skipping: Smallest class has only {min_class_count} sample(s) (too few for CV)")
                return None, None, None, None, False
        
        # FINAL GATEKEEPER: Enforce safety at the last possible moment (ghost buster)
        # This runs AFTER all loading/merging/sanitization is done
        # Extract horizon from target_column or use resolved_config
        target_horizon_minutes = resolved_config.horizon_minutes if resolved_config else None
        if target_horizon_minutes is None and target_column:
            try:
                from TRAINING.ranking.utils.leakage_filtering import _extract_horizon, _load_leakage_config
                leakage_config = _load_leakage_config()
                target_horizon_minutes = _extract_horizon(target_column, leakage_config)
            except Exception:
                pass
        
        # Load config and compute policy cap
        from TRAINING.ranking.utils.leakage_budget import load_lookback_budget_spec, compute_policy_cap_minutes
        spec, warnings = load_lookback_budget_spec("safety_config")
        for warning in warnings:
            logger.warning(f"Config validation: {warning}")
        policy_cap_result = compute_policy_cap_minutes(spec, target_horizon_minutes, detected_interval)
        
        X, feature_names, gate_report = _enforce_final_safety_gate(
            X=X,
            feature_names=feature_names,
            policy_cap_minutes=policy_cap_result.cap_minutes,
            interval_minutes=detected_interval,
            feature_time_meta_map=resolved_config.feature_time_meta_map if resolved_config and hasattr(resolved_config, 'feature_time_meta_map') else None,
            base_interval_minutes=resolved_config.base_interval_minutes if resolved_config and hasattr(resolved_config, 'base_interval_minutes') else None,
            logger=logger
        )
        
        # Update resolved_config with gate_report
        if resolved_config and gate_report.get("enforced_feature_set"):
            resolved_config._gatekeeper_enforced = gate_report["enforced_feature_set"]
        
        # CRITICAL: Quarantine unknown lookback features BEFORE calling compute_feature_lookback_max
        # Invariant: "Budget computation assumes sanitized features"
        # We must filter out features with inf lookback before any budget computation
        if feature_names and len(feature_names) > 0:
            from TRAINING.ranking.utils.leakage_budget import compute_feature_lookback_max, _feat_key
            
            # Step 1: Compute canonical map (non-throwing) to identify unknown features
            # Use a non-strict stage name to avoid raising on unknown features
            lookback_result_precheck = compute_feature_lookback_max(
                feature_names,
                interval_minutes=detected_interval,
                max_lookback_cap_minutes=None,
                stage="shared_harness_pre_budget_check"  # Non-strict stage for pre-check
            )
            canonical_map = lookback_result_precheck.canonical_lookback_map if hasattr(lookback_result_precheck, 'canonical_lookback_map') else {}
            
            # Step 2: Identify and quarantine features with unknown lookback (inf)
            unknown_features = []
            safe_features = []
            unknown_indices = []
            
            for i, feat_name in enumerate(feature_names):
                feat_key = _feat_key(feat_name)
                lookback = canonical_map.get(feat_key)
                
                if lookback is None or lookback == float("inf"):
                    unknown_features.append(feat_name)
                    unknown_indices.append(i)
                else:
                    safe_features.append(feat_name)
            
            # Step 3: Quarantine unknown features (remove from X and feature_names)
            if unknown_features:
                logger.warning(
                    f"  ‚ö†Ô∏è  Quarantining {len(unknown_features)} features with unknown lookback (inf) before budget computation: {unknown_features[:5]}{'...' if len(unknown_features) > 5 else ''}"
                )
                # Remove from X (numpy array)
                if len(unknown_indices) > 0:
                    X = np.delete(X, unknown_indices, axis=1)
                # Update feature_names to only safe features
                feature_names = safe_features
                
                if len(feature_names) == 0:
                    logger.error("‚ùå All features were quarantined due to unknown lookback! Cannot proceed.")
                    return None, None, None, None, False
            
            # Step 4: Filter canonical_map to only include safe features (quarantined features removed)
            # CRITICAL: canonical_map from precheck contains ALL features including unknown ones
            # We must filter it to only include features that survived quarantine
            filtered_canonical_map = {}
            for feat_name in feature_names:  # feature_names already filtered to safe features
                feat_key = _feat_key(feat_name)
                if feat_key in canonical_map:
                    filtered_canonical_map[feat_key] = canonical_map[feat_key]
            canonical_map = filtered_canonical_map  # Use filtered map (guaranteed clean)
            
            # CRITICAL INVARIANT: Verify no inf lookbacks remain after gatekeeper
            n_inf_lookback_after = sum(1 for lookback in canonical_map.values() if lookback == float("inf"))
            if n_inf_lookback_after > 0:
                raise RuntimeError(
                    f"üö® POST_GATEKEEPER INVARIANT VIOLATION: {n_inf_lookback_after} features still have unknown lookback (inf) "
                    f"after quarantine. This indicates a bug - gatekeeper should have removed all inf lookbacks. "
                    f"Sample: {[k for k, v in canonical_map.items() if v == float('inf')][:5]}"
                )
            
            max_lookback_value = max(canonical_map.values()) if canonical_map else None
            max_lookback_str = f"{max_lookback_value:.1f}m" if max_lookback_value is not None else "N/A"
            logger.info(
                f"‚úÖ POST_GATEKEEPER: {len(feature_names)} features, 0 inf lookbacks, "
                f"max_lookback={max_lookback_str}"
            )
            
            # Step 5: Only pass safe features to compute_feature_lookback_max (strict mode)
            if feature_names and len(feature_names) > 0:
                result = compute_feature_lookback_max(
                    feature_names,
                    interval_minutes=detected_interval,
                    max_lookback_cap_minutes=None,
                    stage="shared_harness_post_gatekeeper",
                    canonical_lookback_map=canonical_map  # Use filtered canonical map (only safe features)
                )
                max_lookback_after_gatekeeper = result.max_minutes
                fingerprint = result.fingerprint
                if max_lookback_after_gatekeeper is not None:
                    resolved_config.feature_lookback_max_minutes = max_lookback_after_gatekeeper
                    logger.debug(f"üìä Updated feature_lookback_max after Final Gatekeeper: {max_lookback_after_gatekeeper:.1f}m (from {len(feature_names)} remaining features)")
        
        if X.shape[1] == 0:
            logger.error("‚ùå FINAL GATEKEEPER: All features were dropped! Cannot proceed.")
            return None, None, None, None, False
        
        return X, y, feature_names, resolved_config, True
